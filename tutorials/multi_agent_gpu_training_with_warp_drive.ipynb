{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1349103",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.  \n",
    "All rights reserved.\n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f928d76",
   "metadata": {},
   "source": [
    "### Colab\n",
    "\n",
    "Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/ai-economist/blob/master/tutorials/multi_agent_gpu_training_with_warp_drive.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d85d358",
   "metadata": {},
   "source": [
    "# ⚠️ PLEASE NOTE:\n",
    "This notebook runs on a GPU runtime.\\\n",
    "If running on Colab, choose Runtime > Change runtime type from the menu, then select `GPU` in the 'Hardware accelerator' dropdown menu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073387a4",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome! In this tutorial, we detail how we train multi-agent economic simulations built using [Foundation](https://github.com/salesforce/ai-economist/tree/master/ai_economist/foundation) and train it using [WarpDrive](https://github.com/salesforce/warp-drive), an open-source library we built for extremely fast multi-agent reinforcement learning (MARL) on a single GPU. For the purposes of exposition, we specifically consider the [COVID-19 and economy simulation](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/scenarios/covid19/covid19_env.py). The COVID-19 and economy is a simulation to model health and economy dynamics amidst the COVID-19 pandemic and comprises 52 agents.\n",
    "\n",
    "We put together this tutorial with these goals in mind:\n",
    "- Describe how we train multi-agent simulations from scratch, starting with just a Python implementation of the environment on a CPU.\n",
    "- Provide reference starting code to help perform extremely fast MARL training so the AI Economist community can focus more towards contributing multi-agent simulations to Foundation.\n",
    "\n",
    "We will cover the following concepts:\n",
    "1. Building a GPU-compatible environment.\n",
    "2. CPU-GPU environment consistency checker.\n",
    "3. Adding an *environment wrapper*.\n",
    "4. Creating a *trainer* object, and perform training.\n",
    "5. Generate a rollout using the trainer object and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e761c0d",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "It is helpful to be familiar with [Foundation](https://github.com/salesforce/ai-economist/tree/master/ai_economist/foundation), a multi-agent economic simulator, and also the COVID-19 and Economic simulation ([paper here](https://arxiv.org/abs/2108.02904)). We recommend taking a look at the following tutorials:\n",
    "\n",
    "- [Foundation: the Basics](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb)\n",
    "- [Extending Foundation](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_advanced.ipynb)\n",
    "- [COVID-19 and Economic Simulation](https://github.com/salesforce/ai-economist/blob/master/tutorials/covid19_and_economic_simulation.ipynb)\n",
    "\n",
    "It is also important to get familiarized with [WarpDrive](https://github.com/salesforce/warp-drive), a framework we developed for extremely fast end-to-end reinforcement learning on a single GPU. We also have a detailed tutorial on on how to [create custom environments](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4-create_custom_environments.md) and integrate with WarpDrive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87df34",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bb9c3b",
   "metadata": {},
   "source": [
    "You will need to install the [AI Economist](https://github.com/salesforce/ai-economist) and [WarpDrive](https://github.com/salesforce/warp-drive) pip packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265a4515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/mhelabd/ai-ethicist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bff9430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside covid19_components.py: 1 GPUs are available.\n",
      "Inside covid19_env.py: 1 GPUs are available.\n",
      "Inside env_wrapper.py: 1 GPUs are available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n",
      "/home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/torch/cuda/__init__.py:120: UserWarning: \n",
      "    Found GPU%d %s which is of cuda capability %d.%d.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability supported by this library is %d.%d.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn.format(d, name, major, minor, min_arch // 10, min_arch % 10))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside env_cpu_gpu_consistency_checker.py: 1 GPUs are available.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import dates as mdates\n",
    "from datetime import timedelta\n",
    "from timeit import Timer\n",
    "\n",
    "from ai_economist.foundation.scenarios.moral_economy.moral_dynamic_layout import (\n",
    "    MoralUniform,\n",
    ")\n",
    "from ai_economist.foundation.env_wrapper import FoundationEnvWrapper\n",
    "from ai_economist.foundation.env_cpu_gpu_consistency_checker import EnvironmentCPUvsGPU\n",
    "\n",
    "from warp_drive.training.trainer import Trainer\n",
    "from warp_drive.training.utils.data_loader import create_and_push_data_placeholders\n",
    "from warp_drive.utils.env_registrar import EnvironmentRegistrar\n",
    "\n",
    "# Set font size for the matplotlib figures\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac34e846",
   "metadata": {},
   "source": [
    "# 1. Building a GPU-Compatible Environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cbb4a4",
   "metadata": {},
   "source": [
    "We start with a Python environment that has the [Gym](https://gym.openai.com/docs/)-style `__init__`, `reset` and `step` APIs. For example, consider the [COVID-19 economic simulation](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/scenarios/covid19/covid19_env.py). To build a GPU-compatible environment that can be trained with WarpDrive, you will need to first implement the simulation itself in CUDA C. While there are other alternatives for GPU-based simulations such as [Numba](https://numba.readthedocs.io/en/stable/cuda/index.html) and [JAX](https://jax.readthedocs.io/en/latest/), CUDA C provides the most flexibility for building complex multi-agent simulation logic, and also the fastest performance. However, implementing the simulation in\n",
    "CUDA C also requires the GPU memory and threads to be carefully managed. Some pointers on building and testing the simulation in CUDA C are provided in this WarpDrive [tutorial](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4-create_custom_environments.md).\n",
    "\n",
    "Important: when writing the step function using CUDA C, the function names should follow the following convention so that they can be used with WarpDrive APIs.\n",
    "- The scenario class needs to have a 'name' attribute. The scenario step function requires to be named as \"Cuda{scenario_name}Step\".\n",
    "- Every component class needs to have a 'name' attribute. The step function for the component in the scenario requires to be named as \"Cuda{component_name}Step\".\n",
    "- The function used to compute the rewards requires to be named as \"CudaComputeReward\".\n",
    "\n",
    "The code for the COVID-19 economic simulation's step function is [here](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/scenarios/covid19/covid19_env_step.cu).\n",
    "\n",
    "To use an existing Python Environment with WarpDrive, one needs to add two augmentations (see below) to the Python code. First, a `get_data_dictionary()` method that pushes all the data arrays and environment parameters required to run the simulation to the GPU. Second, the step-function should invoke the `cuda_step` kernel with the data arrays that the CUDA C step function should have access to passed as arguments.\n",
    "\n",
    "```python\n",
    "class Env:\n",
    "    def __init__(self, **env_config):\n",
    "        ...\n",
    "\n",
    "    def reset(self):\n",
    "        ...\n",
    "        return obs\n",
    "\n",
    "    def get_data_dictionary(self):\n",
    "        # Specify the data that needs to be \n",
    "        # pushed to the GPU.\n",
    "        data_feed = DataFeed()\n",
    "        data_feed.add_data(\n",
    "            name=\"variable_name\",\n",
    "            data=self.variable,\n",
    "            save_copy_and_apply_at_reset\n",
    "            =True,\n",
    "        )\n",
    "        ...\n",
    "        return data_feed\n",
    "\n",
    "    def step(self, actions):\n",
    "        if self.use_cuda:\n",
    "            self.cuda_step(\n",
    "                # Pass the relevant data \n",
    "                # feed keys as arguments \n",
    "                # to cuda_step. \n",
    "                # Note: cuda_data_manager \n",
    "                # is created by the \n",
    "                # EnvWrapper.\n",
    "                self.cuda_data_manager.\n",
    "                device_data(...),\n",
    "                ...\n",
    "            )\n",
    "        else:\n",
    "            ...\n",
    "            return obs, rew, done, info\n",
    "```\n",
    "The complete Python code is [here](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/scenarios/covid19/covid19_env.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4645b43",
   "metadata": {},
   "source": [
    "# 2. CPU-GPU Environment Consistency Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9b8ae",
   "metadata": {},
   "source": [
    "Before we train the simulation on the GPU, we will need to ensure consistency between the Python and CUDA C versions of the simulation. For this purpose, Foundation provides an [EnvironmentCPUvsGPU class](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/env_cpu_gpu_consistency_checker.py). This module essentially instantiates environment objects corresponding to the two versions of the simulation. It then steps through the two environment objects for a specified number of environment replicas `num_envs` and a specified number of episodes `num_episodes`, and verifies that the observations, actions, rewards and the “done” flags are the same after each step. We have created a testing [script](https://github.com/salesforce/ai-economist/blob/master/tests/run_covid19_cpu_gpu_consistency_checks.py) that performs the consistency checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a990b032",
   "metadata": {},
   "source": [
    "First, we will create an environment configuration to test with. For more details on what the configuration parameters mean, please refer to the simulation [code](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/scenarios/covid19/covid19_env.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c331cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 1\n",
    "ethics = 'utilitarian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315d41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a configuration (dictionary) for the \"gather-trade-build\" environment.\n",
    "\n",
    "env_config_dict = {\n",
    "    # ===== SCENARIO CLASS =====\n",
    "    # Which Scenario class to use: the class's name in the Scenario Registry (foundation.scenarios).\n",
    "    # The environment object will be an instance of the Scenario class.\n",
    "    'scenario_name': 'moral_uniform/simple_wood_and_stone',\n",
    "#         ===== MORALITY =====\n",
    "    'moral_theory': ethics, \n",
    "    'agent_morality': lambd, \n",
    "#     'scenario_name': 'uniform/simple_wood_and_stone',\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ===== COMPONENTS =====\n",
    "    # Which components to use (specified as list of (\"component_name\", {component_kwargs}) tuples).\n",
    "    #   \"component_name\" refers to the Component class's name in the Component Registry (foundation.components)\n",
    "    #   {component_kwargs} is a dictionary of kwargs passed to the Component class\n",
    "    # The order in which components reset, step, and generate obs follows their listed order below.\n",
    "    'components': [\n",
    "        # (1) Building houses\n",
    "        ('Build', {\n",
    "            'skill_dist':                   'pareto', \n",
    "            'payment_max_skill_multiplier': 3,\n",
    "            'build_labor':                  10,\n",
    "            'payment':                      10\n",
    "        }),\n",
    "        # (2) Trading collectible resources\n",
    "        ('ContinuousDoubleAuction', {\n",
    "            'max_bid_ask':    10,\n",
    "            'order_labor':    0.25,\n",
    "            'max_num_orders': 5,\n",
    "            'order_duration': 50\n",
    "        }),\n",
    "        # (3) Movement and resource collection\n",
    "        ('Gather', {\n",
    "            'move_labor':    1,\n",
    "            'collect_labor': 1,\n",
    "            'skill_dist':    'pareto'\n",
    "        }),\n",
    "#         (4) Stealing,\n",
    "        ('Steal', {\n",
    "            'steal_labor':    1,\n",
    "            'skill_dist':    'pareto'  \n",
    "        }),\n",
    "    ],\n",
    "    \n",
    "    \n",
    "    # ===== SCENARIO CLASS ARGUMENTS =====\n",
    "    # (optional) kwargs that are added by the Scenario class (i.e. not defined in BaseEnvironment)\n",
    "    'starting_agent_coin': 10,\n",
    "    \n",
    "    # ===== STANDARD ARGUMENTS ======\n",
    "    # kwargs that are used by every Scenario class (i.e. defined in BaseEnvironment)\n",
    "    'n_agents': 4,          # Number of non-planner agents (must be > 1)\n",
    "    'world_size': [25, 25], # [Height, Width] of the env world\n",
    "    'episode_length': 1000, # Number of timesteps per episode\n",
    "    \n",
    "    # In multi-action-mode, the policy selects an action for each action subspace (defined in component code).\n",
    "    # Otherwise, the policy selects only 1 action.\n",
    "    'multi_action_mode_agents': False,\n",
    "    'multi_action_mode_planner': True,\n",
    "    \n",
    "    # When flattening observations, concatenate scalar & vector observations before output.\n",
    "    # Otherwise, return observations with minimal processing.\n",
    "    'flatten_observations': True,\n",
    "    # When Flattening masks, concatenate each action subspace mask into a single array.\n",
    "    # Note: flatten_masks = True is required for masking action logits in the code below.\n",
    "    'flatten_masks': True,\n",
    "    \n",
    "    # How often to save the dense logs\n",
    "    'dense_log_frequency': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6bd56b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EnvWrapper] Spaces\n",
      "[EnvWrapper] Obs (a)   \n",
      "action_mask    : (51,)\n",
      "flat           : (122,)\n",
      "time           : (1,)\n",
      "world-idx_map  : (2, 11, 11)\n",
      "world-map      : (6, 11, 11)\n",
      "\n",
      "\n",
      "[EnvWrapper] Obs (p)   \n",
      "action_mask    : (1,)\n",
      "flat           : (72,)\n",
      "p0             : (5,)\n",
      "p1             : (5,)\n",
      "p2             : (5,)\n",
      "p3             : (5,)\n",
      "time           : (1,)\n",
      "world-idx_map  : (2, 25, 25)\n",
      "world-map      : (5, 25, 25)\n",
      "\n",
      "\n",
      "[EnvWrapper] Action (a) Discrete(51)\n",
      "[EnvWrapper] Action (p) MultiDiscrete([1])\n"
     ]
    }
   ],
   "source": [
    "from rllib.env_wrapper import RLlibEnvWrapper\n",
    "env_obj = RLlibEnvWrapper({\"env_config_dict\": env_config_dict}, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364047c4",
   "metadata": {},
   "source": [
    "Next, we will need to register the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70f5ad8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mhelabd/ai-ethicist/ai_economist/foundation/scenarios/moral_uniform/moral_dynamic_layout.cu'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_registrar.get_cuda_env_src_path(MoralUniform.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f5f0e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:EnvironmentRegistrar has already registered an environment path called moral_uniform/simple_wood_and_stone but we will re-register it by overwriting the previous source code path\n"
     ]
    }
   ],
   "source": [
    "env_registrar = EnvironmentRegistrar()\n",
    "env_registrar.add_cuda_env_src_path(\n",
    "    MoralUniform.name,\n",
    "    '/home/mhelabd/ai-ethicist/ai_economist/foundation/scenarios/moral_uniform/moral_dynamic_layout.cu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213dbdf0",
   "metadata": {},
   "source": [
    "The consistency tests may be performed using the `test_env_reset_and_step()` API as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b12b4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_includes/env_runner.cu\n",
    "!rm /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_includes/env_config.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb67844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1898dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:the destination header file /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_includes/env_config.h already exists; remove and rebuild.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test...\n",
      "Initializing the CUDA data manager...\n",
      "Initializing the CUDA function manager...\n",
      "compile_and_load_cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:build failed when running the following build... : \n",
      "nvcc --fatbin -arch=sm_61 /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_includes/env_runner.cu -o /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_bin/env_runner.fatbin \n",
      "try to build the fatbin hybrid version of virtual PTX + gpu binary ... \n",
      "ERROR:root:build failed when running the following build... : \n",
      "nvcc --fatbin -arch=compute_37 -code=compute_37 -code=sm_37 -code=sm_50 -code=sm_60 -code=sm_70 -code=sm_80 /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_includes/env_runner.cu -o /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_bin/env_runner.fatbin \n",
      "try to build the lower gpu-code version ... \n",
      "ERROR:root:build failed when running the following build... : \n",
      "nvcc --fatbin -arch=compute_37 -code=compute_37 -code=sm_37 -code=sm_50 -code=sm_60 -code=sm_70 /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_includes/env_runner.cu -o /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_bin/env_runner.fatbin \n",
      "try to build the lower gpu-code version ... \n",
      "ERROR:root:build failed when running the following build... : \n",
      "nvcc --fatbin -arch=compute_37 -code=compute_37 -code=sm_37 -code=sm_50 -code=sm_60 /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_includes/env_runner.cu -o /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_bin/env_runner.fatbin \n",
      "try to build the lower gpu-code version ... \n",
      "ERROR:root:build failed when running the following build... : \n",
      "nvcc --fatbin -arch=compute_37 -code=compute_37 -code=sm_37 -code=sm_50 /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_includes/env_runner.cu -o /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_bin/env_runner.fatbin \n",
      "try to build the lower gpu-code version ... \n",
      "ERROR:root:build failed when running the following build... : \n",
      "nvcc --fatbin -arch=compute_37 -code=compute_37 -code=sm_37 /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_includes/env_runner.cu -o /home/mhelabd/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/cuda_bin/env_runner.fatbin \n",
      "try to build the lower gpu-code version ... \n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "build failed ... ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9591e9585074>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcustomized_env_registrar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_registrar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m ).test_env_reset_and_step()\n",
      "\u001b[0;32m~/ai-ethicist/ai_economist/foundation/env_cpu_gpu_consistency_checker.py\u001b[0m in \u001b[0;36mtest_env_reset_and_step\u001b[0;34m(self, consistency_threshold_pct)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mnum_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mcustomized_env_registrar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustomized_env_registrar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             )\n\u001b[1;32m    149\u001b[0m             \u001b[0menv_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_all_envs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ai-ethicist/ai_economist/foundation/env_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_obj, num_envs, use_cuda, customized_env_registrar)\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mtemplate_header_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"template_env_config.h\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mtemplate_runner_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"template_env_runner.cu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mcustomized_env_registrar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustomized_env_registrar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             )\n\u001b[1;32m    205\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initialize_functions...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/managers/function_manager.py\u001b[0m in \u001b[0;36mcompile_and_load_cuda\u001b[0;34m(self, env_name, template_header_file, template_runner_file, template_path, default_functions_included, customized_env_registrar, event_messenger)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Compiling {main_file} -> {cubin_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcubin_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent_messenger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/managers/function_manager.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(main_file, cubin_file)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuild_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"build failed ... \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize_default_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: build failed ... "
     ]
    }
   ],
   "source": [
    "env_config_dict.pop('scenario_name', None)\n",
    "EnvironmentCPUvsGPU(\n",
    "    env_class=MoralUniform,\n",
    "    env_configs={\"test\": env_config_dict},\n",
    "    num_envs=3,\n",
    "    num_episodes=2,\n",
    "    customized_env_registrar=env_registrar,\n",
    ").test_env_reset_and_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867963d",
   "metadata": {},
   "source": [
    "If the two implementations are consistent, you should see  `The CPU and the GPU environment outputs are consistent within 1 percent` at the end of the previous cell run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83bf539",
   "metadata": {},
   "source": [
    "# 3. Adding an *Environment Wrapper*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7f438",
   "metadata": {},
   "source": [
    "Once the Python and CUDA C implementation are consistent with one another, we use an environment wrapper to wrap the environment object, and run the simulation on the GPU. Accordingly, we need to set the `use_cuda` argument to True. under this setting, only the first environment reset happens on the CPU. Following that, the data arrays created at reset and the simulation parameters are copied over (a one-time operation) to the GPU memory. All the subsequent steps (and resets) happen only on the GPU. In other words, there's no back-and-forth data copying between the CPU and the GPU, and all the data arrays on the GPU are modified in-place. The environment wrapper also uses the `num_envs` argument (defaults to $1$) to instantiate multiple replicas of the environment on the GPU.\n",
    "\n",
    "Note: for running the simulation on a CPU, simply set use_cuda=False, and it is no different than actually running the Python simulation on a CPU - the reset and step calls also happen on the CPU.\n",
    "\n",
    "The environment wrapper essentially performs the following tasks that are required to run the simulation on the GPU:\n",
    "- Registers the CUDA step kernel, so that the step function can be invoked from the CPU (host).\n",
    "- Pushes all the data listed in the data dictionary to the GPU when the environment is reset for the very frst time.\n",
    "- Automatically resets every environment when it reaches its done state.\n",
    "- Adds the observation and action spaces to the environment, which are required when training the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede05b74",
   "metadata": {},
   "source": [
    "The CPU and GPU versions of the environment object may be created via setting the appropriate value of the `use_cuda` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77db1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_env = FoundationEnvWrapper(\n",
    "    MoralUniform(**env_config_dict),\n",
    "    use_cuda=False,\n",
    "    customized_env_registrar=env_registrar,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa259b4e",
   "metadata": {},
   "source": [
    "Instantiating the GPU environment also initializes the data function managers and loads the CUDA kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e433e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_env = FoundationEnvWrapper(\n",
    "    MoralUniform(**env_config_dict),\n",
    "    use_cuda=True,\n",
    "    customized_env_registrar=env_registrar,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204d966",
   "metadata": {},
   "source": [
    "# 4. Creating a Trainer Object and Perform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbea778",
   "metadata": {},
   "source": [
    "Next, we will prepare the environment for training on a GPU.We will need to define a run configuration (which comprises the environment, training, policy and saving configurations), and create a *trainer* object.\n",
    "\n",
    "We will load the run configuration from a saved yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c41ee947",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(\n",
    "    os.path.dirname(os.path.abspath(\"__file__\")),\n",
    "    \"./ai_economist/training/run_configs/\",\n",
    "    f\"moral_economy.yaml\",\n",
    ")\n",
    "with open(config_path, \"r\", encoding=\"utf8\") as f:\n",
    "    run_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65735b1",
   "metadata": {},
   "source": [
    "Next, we will create and instantiate the trainer object. To this end, we will need to set some arguments: policy_tag_to_agent_id_map, separate_placeholder_per_policy (defaults to False) and obs_dim_corresponding_to_num_agents (defaults to \"first\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "544a2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy_tag_to_agent_id_map dictionary maps\n",
    "# policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"a\": [str(agent_id) for agent_id in range(cpu_env.env.n_agents)],\n",
    "    \"p\": [\"p\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95a55dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag indicating whether separate obs, actions and rewards placeholders have to be created for each policy.\n",
    "# Set \"create_separate_placeholders_for_each_policy\" to True here \n",
    "# since the agents and the planner have different observation and action spaces.\n",
    "separate_placeholder_per_policy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e43f24c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag indicating the observation dimension corresponding to 'num_agents'.\n",
    "# Note: WarpDrive assumes that all the observation are shaped\n",
    "# (num_agents, *feature_dim), i.e., the observation dimension\n",
    "# corresponding to 'num_agents' is the first one. Instead, if the\n",
    "# observation dimension corresponding to num_agents is the last one,\n",
    "# we will need to permute the axes to align with WarpDrive's assumption\n",
    "obs_dim_corresponding_to_num_agents = \"last\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed835bbd",
   "metadata": {},
   "source": [
    "### Instantiating the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97f241ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FoundationEnvWrapper' object has no attribute 'cuda_function_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-52391daecf4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpolicy_tag_to_agent_id_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_tag_to_agent_id_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcreate_separate_placeholders_for_each_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparate_placeholder_per_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mobs_dim_corresponding_to_num_agents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs_dim_corresponding_to_num_agents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/ai-ethicist/lib/python3.7/site-packages/warp_drive/training/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_wrapper, config, policy_tag_to_agent_id_map, create_separate_placeholders_for_each_policy, obs_dim_corresponding_to_num_agents, device_id, num_devices, results_dir, verbose)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_batch_size_per_env\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_envs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_function_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_envs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_data_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_envs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_function_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FoundationEnvWrapper' object has no attribute 'cuda_function_manager'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    env_wrapper=cpu_env,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    "    create_separate_placeholders_for_each_policy=separate_placeholder_per_policy,\n",
    "    obs_dim_corresponding_to_num_agents=obs_dim_corresponding_to_num_agents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb37f72",
   "metadata": {},
   "source": [
    "### CPU vs GPU Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a582b81",
   "metadata": {},
   "source": [
    "Before performing training, let us see how the simulation speed on the GPU compares with that of the CPU. We will generate a set of random actions, and step through both versions of the simulation a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fdb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_actions(env):\n",
    "    actions = {\n",
    "            str(agent_id): np.random.randint(\n",
    "                low=0,\n",
    "                high=env.env.action_space[str(agent_id)].n,\n",
    "                dtype=np.int32,\n",
    "            )\n",
    "            for agent_id in range(env.n_agents-1)\n",
    "    }\n",
    "    actions[\"p\"] = np.random.randint(\n",
    "        low=0,\n",
    "        high=env.env.action_space[\"p\"].n,\n",
    "        dtype=np.int32,\n",
    "    )\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_reset_and_step(env):\n",
    "    env.reset()\n",
    "    actions = generate_random_actions(env)\n",
    "    for t in range(env.episode_length):\n",
    "        env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd1cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "Timer(lambda: env_reset_and_step(gpu_env)).timeit(number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Timer(lambda: env_reset_and_step(cpu_env)).timeit(number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626a15a",
   "metadata": {},
   "source": [
    "Notice that with just $1$ replica of the environment, the environment step on the GPU is over 5x faster (on an A100 machine). When running training, it is typical to use several environment replicas, and that provides an even higher performance boost for the GPU, since WarpDrive runs all the environment replicas in parallel on separate GPU blocks, and the CPU cannot achieve the same amount of parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6846a",
   "metadata": {},
   "source": [
    "### Perform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0aac10",
   "metadata": {},
   "source": [
    "We perform training by invoking `trainer.train()`. The speed performance stats and metrics for the trained policies are printed on screen.\n",
    "Note: In this notebook, we only run training for $200$ iterations. You may run it for longer by setting the `num_episodes` configuration parameter [here](https://github.com/salesforce/ai-economist/blob/master/ai_economist/training/run_configs/covid_and_economy_environment.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71acf9",
   "metadata": {},
   "source": [
    "# 5. Visualize the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0973cb",
   "metadata": {},
   "source": [
    "Post training, it is useful to visualize some of the environment's actions and observations to gain more insight into the kinds of policies the RL agents learn, and the resulting environment dynamics. For the COVID-19 and economic simulation, the actions - \"stringency level\" and \"subsidy level\" control the observables such such as \"susceptible\", \"infected\", \"recovered\", \"deaths\", \"unemployed\", \"vaccinated\" and \"productivity\" for each of the US states.\n",
    "\n",
    "Incidentally, these actions and observables also correspond to the names of arrays that were pushed to the GPU after the very first environment reset. At any time, the arrays can be fetched back to the CPU via the WarpDrive trainer's API `fetch_episode_states`, and visualized for the duration of an entire episode. Below, we also provide a helper function to perform the visualizations. Note that in this notebook, we only performed a few iterations of training, so the policies will not be quite trained at this point, so the plots seen in the visualization seen are going to be arbitrary. You may run training with a different set of configurations or for longer by setting the `num_episodes` configuration parameter [here](https://github.com/salesforce/ai-economist/blob/master/ai_economist/training/run_configs/covid_and_economy_environment.yaml), and you can visualize the policies after, using the same code provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f755a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: visualizations\n",
    "\n",
    "def visualize_states(\n",
    "    entity=\"USA\",\n",
    "    episode_states=None,\n",
    "    trainer=None,\n",
    "    ax=None\n",
    "):\n",
    "    assert trainer is not None\n",
    "    assert episode_states is not None\n",
    "       \n",
    "    # US state names to index mapping\n",
    "    us_state_name_to_idx = {v: k for k, v in trainer.cuda_envs.env.us_state_idx_to_state_name.items()}\n",
    "    us_state_name_to_idx[\"USA\"] = \"p\"\n",
    "    agent_id = us_state_name_to_idx[entity]\n",
    "    \n",
    "    assert entity is not None\n",
    "    assert entity in us_state_name_to_idx.keys(), f\"entity should be in {list(us_state_name_to_idx.keys())}\"\n",
    "    \n",
    "    for key in episode_states:\n",
    "        # Use the collated data at the last valid time index\n",
    "        last_valid_time_index = np.isnan(np.sum(episode_states[key], axis = (1, 2))).argmax() - 1\n",
    "        episode_states[key] = episode_states[key][last_valid_time_index]\n",
    "    \n",
    "    if agent_id == \"p\":\n",
    "        for key in episode_states:\n",
    "                \n",
    "            if key in [\"subsidy_level\", \"stringency_level\"]:\n",
    "                episode_states[key] = np.mean(episode_states[key], axis=-1) # average across all the US states\n",
    "            else:\n",
    "                episode_states[key] = np.sum(episode_states[key], axis=-1) # sum across all the US states\n",
    "            episode_states[key] = episode_states[key].reshape(-1, 1) # putting back the agent_id dimension\n",
    "        agent_id = 0\n",
    "    else:\n",
    "        agent_id = int(agent_id)\n",
    "        \n",
    "    if ax is None:\n",
    "        if len(episode_states) < 3:\n",
    "            cols = len(episode_states)\n",
    "        else:\n",
    "            cols = 3\n",
    "        scale = 8\n",
    "        rows = int(np.ceil(len(episode_states) / cols))\n",
    "    \n",
    "        h, w = scale*max(rows, cols), scale*max(rows, rows)\n",
    "        fig, ax = plt.subplots(rows, cols, figsize=(h, w), sharex=True, squeeze=False)\n",
    "    else:\n",
    "        rows, cols = ax.shape\n",
    "        \n",
    "    start_date = trainer.cuda_envs.env.start_date\n",
    "    \n",
    "    for idx, key in enumerate(episode_states):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "\n",
    "        dates = [start_date + timedelta(day) for day in range(episode_states[key].shape[0])]\n",
    "        ax[row][col].plot(dates, episode_states[key][:, agent_id], linewidth=3)\n",
    "        ax[row][col].set_ylabel(key)\n",
    "        ax[row][col].grid(b=True)\n",
    "        ax[row][col].xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "        ax[row][col].xaxis.set_major_formatter(mdates.DateFormatter(\"%b'%y\"))\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa83589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the key state indicators for an episode.\n",
    "episode_states = trainer.fetch_episode_states(\n",
    "    [\n",
    "        \"stringency_level\",\n",
    "        \"subsidy_level\",        \n",
    "        \"susceptible\",\n",
    "        \"infected\",\n",
    "        \"recovered\",\n",
    "        \"deaths\",\n",
    "        \"unemployed\",\n",
    "        \"vaccinated\",\n",
    "        \"productivity\",\n",
    "        \"postsubsidy_productivity\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Visualize the fetched states.\n",
    "# Feel free to modify the 'entity' argument to visualize the curves for the US states (e.g., California, Utah) too.\n",
    "visualize_states(\n",
    "    entity=\"USA\",\n",
    "    episode_states=episode_states,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649b9c5",
   "metadata": {},
   "source": [
    "And that's it for this tutorial. Happy training with Foundation and WarpDrive!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
